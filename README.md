# ğŸŒ LLM API Gateway ğŸš€

![LLM API Gateway Banner](https://github.com/simonpierreboucher0/LLM-API/raw/main/assets/banner.png)

## ğŸ”¥ La passerelle universelle vers tous vos modÃ¨les de langage prÃ©fÃ©rÃ©s ğŸ”¥

[![GitHub stars](https://img.shields.io/github/stars/simonpierreboucher0/LLM-API?style=social)](https://github.com/simonpierreboucher0/LLM-API/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.105.0+-green.svg)](https://fastapi.tiangolo.com/)

---

## âœ¨ CaractÃ©ristiques principales

ğŸ”„ **Interface unifiÃ©e** - Une API pour tous les fournisseurs  
ğŸ§© **Normalisation automatique** - Adaptation transparente entre tous les modÃ¨les  
ğŸ”Œ **CompatibilitÃ© OpenAI** - RÃ©ponses au format standardisÃ©  
âš¡ **Streaming temps rÃ©el** - RÃ©ponses fluides token par token  
ğŸ›¡ï¸ **Gestion des spÃ©cificitÃ©s** - Adaptation aux particularitÃ©s de chaque modÃ¨le  
ğŸ“Š **Logging avancÃ©** - Suivi dÃ©taillÃ© des requÃªtes  
ğŸ”’ **SÃ©curitÃ© intÃ©grÃ©e** - Gestion sÃ©curisÃ©e des clÃ©s API  
ğŸ§  **+100 modÃ¨les** - AccÃ¨s Ã  une vaste bibliothÃ¨que de LLMs  

---

## ğŸ¤– Fournisseurs et modÃ¨les pris en charge

### ğŸŸ¢ OpenAI
ModÃ¨les phares du leader du marchÃ© des LLMs.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸ† **gpt-4o** | Le fleuron multimodal d'OpenAI | Applications premium, contenu crÃ©atif, rÃ©solution de problÃ¨mes complexes |
| ğŸ§  **gpt-4** | ModÃ¨le fondateur de la sÃ©rie GPT-4 | Raisonnement, analyse, gÃ©nÃ©ration de contenu de qualitÃ© |
| ğŸ’¨ **gpt-3.5-turbo** | ModÃ¨le rapide et Ã©conomique | Chatbots, assistance client, applications grand public |
| ğŸ“š **gpt-4-0613** | Version spÃ©cifique de GPT-4 | Applications nÃ©cessitant une version stable et spÃ©cifique |
| âš¡ **gpt-4-turbo-2024-04-09** | Version turbo de GPT-4 | Traitement plus rapide, connaissances plus rÃ©centes |
| ğŸ“¦ **gpt-3.5-turbo-0125** | Version spÃ©cifique de GPT-3.5 | Applications nÃ©cessitant une version stable et spÃ©cifique |
| ğŸ’ **gpt-4o-mini-2024-07-18** | Version lÃ©gÃ¨re de GPT-4o | Applications avec contraintes de coÃ»t ou de latence |
| ğŸš€ **gpt-4o-2024-08-06** | Version rÃ©cente de GPT-4o | Applications nÃ©cessitant les derniÃ¨res amÃ©liorations |
| ğŸŒŸ **gpt-4o-2024-05-13** | Version stable de GPT-4o | Applications nÃ©cessitant fiabilitÃ© et stabilitÃ© |
| ğŸ§ª **o1** | ModÃ¨le expÃ©rimental avancÃ© | Raisonnement profond, problÃ¨mes complexes, recherche |
| ğŸ”¬ **o3-mini** | Version compacte du modÃ¨le expÃ©rimental | Raisonnement avancÃ© avec contraintes de ressources |

### ğŸŸ£ Anthropic
ModÃ¨les conÃ§us pour la sÃ©curitÃ© et l'alignement Ã©thique.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸŒ  **claude-3-5-sonnet-20241022** | DerniÃ¨re version de Claude 3.5 Sonnet | Equilibre performance/coÃ»t, applications professionnelles |
| âš¡ **claude-3-5-haiku-20241022** | Version rapide de Claude 3.5 | Applications nÃ©cessitant faible latence, assistants en temps rÃ©el |
| ğŸ”® **claude-3-opus-20240229** | ModÃ¨le le plus puissant de Claude | Recherche, analyses complexes, gÃ©nÃ©ration de contenu premium |
| ğŸ“ **claude-3-sonnet-20240229** | Version Ã©quilibrÃ©e de Claude 3 | Applications professionnelles, contenu de qualitÃ© |
| ğŸš€ **claude-3-haiku-20240307** | Version lÃ©gÃ¨re et rapide | Chatbots rapides, applications mobiles, interactions temps rÃ©el |
| ğŸ’« **claude-3-7-sonnet-20250219** | Version amÃ©liorÃ©e et avancÃ©e | Applications nÃ©cessitant les derniÃ¨res innovations |

### ğŸ”µ Mistral AI
ModÃ¨les europÃ©ens Ã  la pointe de la performance.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸŒŠ **mistral-large-latest** | Meilleur modÃ¨le gÃ©nÃ©ral de Mistral | Applications premium nÃ©cessitant haute qualitÃ© |
| ğŸ¨ **pixtral-large-latest** | Variante multimodale avancÃ©e | Applications traitant des images et du texte |
| ğŸ”¹ **ministral-3b-latest** | ModÃ¨le ultra-compact (3B) | Applications mobiles, edge computing, faibles ressources |
| ğŸ”· **ministral-8b-latest** | ModÃ¨le compact (8B) | Applications avec ressources limitÃ©es mais besoin de qualitÃ© |
| ğŸ§¬ **open-mistral-nemo** | Version open-source optimisÃ©e | Applications open-source, dÃ©ploiements personnalisÃ©s |
| ğŸ”µ **mistral-small-latest** | Version Ã©quilibrÃ©e performance/coÃ»t | Applications commerciales, chatbots d'entreprise |
| ğŸŒ€ **mistral-saba-latest** | ModÃ¨le spÃ©cialisÃ© nouvelle gÃ©nÃ©ration | Applications nÃ©cessitant des capacitÃ©s avancÃ©es spÃ©cifiques |
| ğŸ’» **codestral-latest** | SpÃ©cialisÃ© pour la programmation | GÃ©nÃ©ration de code, dÃ©bogage, assistance dÃ©veloppeurs |

### ğŸŸ  Cohere
ModÃ¨les optimisÃ©s pour la recherche et le traitement d'informations.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸŒ **c4ai-aya-expanse-32b** | ModÃ¨le multilingue avancÃ© (32B) | Applications globales, traduction, comprÃ©hension multiculturelle |
| ğŸŒ **c4ai-aya-expanse-8b** | Version compacte multilingue (8B) | Applications multilingues lÃ©gÃ¨res, traduction rapide |
| ğŸ§­ **command** | ModÃ¨le principal de Cohere | Applications gÃ©nÃ©rales, chatbots, gÃ©nÃ©ration de texte |
| ğŸ”† **command-light** | Version lÃ©gÃ¨re de Command | Applications avec contraintes de ressources |
| ğŸŒ“ **command-light-nightly** | Version nightly de Command Light | Tests des derniÃ¨res amÃ©liorations |
| ğŸŒ’ **command-nightly** | Version nightly de Command | AccÃ¨s aux fonctionnalitÃ©s expÃ©rimentales |
| ğŸ“¡ **command-r** | Version avec capacitÃ©s de recherche | Applications nÃ©cessitant recherche et synthÃ¨se d'informations |
| ğŸ“… **command-r-08-2024** | Version datÃ©e de Command-R | Applications nÃ©cessitant une version stable spÃ©cifique |
| ğŸ“¡âš¡ **command-r-plus** | Version amÃ©liorÃ©e de Command-R | Applications premium nÃ©cessitant recherche avancÃ©e |
| ğŸ“…âš¡ **command-r-plus-08-2024** | Version datÃ©e de Command-R Plus | StabilitÃ© et performance pour applications critiques |
| ğŸ”µ **command-r7b-12-2024** | Version compacte spÃ©cifique (7B) | Applications lÃ©gÃ¨res avec recherche intÃ©grÃ©e |
| ğŸŒ™ **command-r7b-arabic-02-2025** | SpÃ©cialisÃ© pour l'arabe | Applications ciblant le marchÃ© arabophone |

### ğŸŸ¤ DeepSeek
ModÃ¨les spÃ©cialisÃ©s dans le raisonnement avancÃ©.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸ’¬ **deepseek-chat** | ModÃ¨le conversationnel principal | Chatbots intelligents, assistants personnels |
| ğŸ§® **deepseek-reasoner** | SpÃ©cialisÃ© dans le raisonnement | ProblÃ¨mes logiques, mathÃ©matiques, rÃ©solution structurÃ©e |

### âš« xAI
ModÃ¨les de la sociÃ©tÃ© d'Elon Musk.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸ”® **grok-2** | ModÃ¨le conversationnel avancÃ© | Assistants intelligents avec personnalitÃ©, applications interactives |

### ğŸ”´ Google
ModÃ¨les multimodaux de pointe.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸŒˆ **gemini-1.5-pro** | ModÃ¨le multimodal premium | Applications multimodales avancÃ©es, analyse d'images et vidÃ©os |
| âš¡ **gemini-1.5-flash** | Version rapide de Gemini | Applications en temps rÃ©el, traitement multimodal efficace |
| ğŸš€ **gemini-1.5-flash-8b** | Version compacte (8B) | Applications multimodales avec contraintes de ressources |

### ğŸŸ¡ Qwen (Alibaba)
ModÃ¨les multilingues excellant en chinois et anglais.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸ‘‘ **qwen-max** | ModÃ¨le le plus puissant de Qwen | Applications premium, analyses complexes |
| â­ **qwen-plus** | Version Ã©quilibrÃ©e haut de gamme | Applications professionnelles, bon rapport qualitÃ©/prix |
| ğŸ”† **qwen-turbo** | Version optimisÃ©e pour la vitesse | Chatbots rapides, applications en temps rÃ©el |
| ğŸ”¹ **qwen2.5-14b-instruct-1m** | Qwen 2.5 (14B) avec 1M contexte | Applications nÃ©cessitant longue mÃ©moire contextuelle |
| ğŸ”¸ **qwen2.5-7b-instruct-1m** | Qwen 2.5 (7B) avec 1M contexte | Applications lÃ©gÃ¨res avec long contexte |
| ğŸ’ **qwen2.5-72b-instruct** | Plus grand modÃ¨le Qwen 2.5 (72B) | Applications premium nÃ©cessitant haute qualitÃ© |
| ğŸ’« **qwen2.5-32b-instruct** | ModÃ¨le intermÃ©diaire (32B) | Bon Ã©quilibre performance/ressources |
| ğŸŒŸ **qwen2.5-14b-instruct** | ModÃ¨le compact (14B) | Applications commerciales standard |
| âœ¨ **qwen2.5-7b-instruct** | ModÃ¨le lÃ©ger (7B) | Applications avec contraintes de ressources |
| ğŸ”… **qwen2-72b-instruct** | Qwen 2 version large (72B) | Applications premium gÃ©nÃ©ration prÃ©cÃ©dente |
| ğŸ”† **qwen2-7b-instruct** | Qwen 2 version compacte (7B) | Applications lÃ©gÃ¨res gÃ©nÃ©ration prÃ©cÃ©dente |
| ğŸ‘‘ **qwen1.5-110b-chat** | Plus grand modÃ¨le Qwen 1.5 (110B) | Applications trÃ¨s exigeantes en qualitÃ© |
| ğŸ’« **qwen1.5-72b-chat** | ModÃ¨le large Qwen 1.5 (72B) | Applications premium gÃ©nÃ©ration prÃ©cÃ©dente |
| â­ **qwen1.5-32b-chat** | ModÃ¨le intermÃ©diaire (32B) | Applications commerciales standard |
| ğŸŒŸ **qwen1.5-14b-chat** | ModÃ¨le compact (14B) | Applications avec ressources modÃ©rÃ©es |
| âœ¨ **qwen1.5-7b-chat** | ModÃ¨le lÃ©ger (7B) | Applications avec contraintes importantes |

### ğŸŸ¦ AI21
ModÃ¨les spÃ©cialisÃ©s dans la gÃ©nÃ©ration structurÃ©e et contrainte.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸ† **jamba-1.5-large** | ModÃ¨le premium de AI21 | Applications professionnelles, gÃ©nÃ©ration structurÃ©e |
| ğŸš€ **jamba-1.5-mini** | Version compacte de Jamba | Applications avec contraintes de ressources |

### ğŸŸª Perplexity
ModÃ¨les optimisÃ©s pour la recherche et la synthÃ¨se d'informations.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸ”¬ **sonar-deep-research** | ModÃ¨le spÃ©cialisÃ© recherche approfondie | Recherche acadÃ©mique, analyses complexes |
| ğŸ§  **sonar-reasoning-pro** | Premium avec capacitÃ©s de raisonnement | Applications nÃ©cessitant analyse logique avancÃ©e |
| ğŸ’­ **sonar-reasoning** | CapacitÃ©s de raisonnement standard | Applications nÃ©cessitant analyse logique |
| â­ **sonar-pro** | Version premium de Sonar | Applications professionnelles et commerciales |
| ğŸ” **sonar** | ModÃ¨le de base pour la recherche | Applications standard de recherche et synthÃ¨se |
| ğŸ‡ºğŸ‡¸ **r1-1776** | ModÃ¨le spÃ©cialisÃ© US-centric | Applications ciblant le marchÃ© amÃ©ricain |

### ğŸŸ¨ DeepInfra
Plateforme offrant de nombreux modÃ¨les open-source optimisÃ©s.

| ModÃ¨le | Description | Cas d'utilisation |
|--------|-------------|-------------------|
| ğŸŒŠ **mistralai/Mixtral-8x7B-Instruct-v0.1** | Mixtral MoE optimisÃ© | Applications nÃ©cessitant expertise variÃ©e |
| ğŸ§¬ **mistralai/Mistral-Nemo-Instruct-2407** | Mistral optimisÃ© pour Nemo | Applications spÃ©cifiques Mistral |
| ğŸš€ **mistralai/Mistral-7B-Instruct-v0.3** | Mistral compact (7B) | Applications efficientes et lÃ©gÃ¨res |
| ğŸ¦™ **meta-llama/Meta-Llama-3-8B-Instruct** | Llama 3 compact | Applications avec ressources limitÃ©es |
| ğŸª **meta-llama/Meta-Llama-3-70B-Instruct** | Llama 3 grand modÃ¨le | Applications premium nÃ©cessitant haute qualitÃ© |
| ğŸ¦™ğŸ”¹ **meta-llama/Llama-3.2-3B-Instruct** | Ultra-compact Llama 3.2 | Applications trÃ¨s lÃ©gÃ¨res, edge computing |
| ğŸ¦™ğŸ”¸ **meta-llama/Llama-3.2-1B-Instruct** | Nano Llama 3.2 (1B) | Applications embarquÃ©es, ressources minimales |
| ğŸ’ **google/gemma-2-9b-it** | Gemma 2 compact | Applications lÃ©gÃ¨res avec bonne qualitÃ© |
| ğŸŒŸ **google/gemma-2-27b-it** | Gemma 2 intermÃ©diaire | Applications professionnelles Ã©quilibrÃ©es |
| ğŸŒŒ **Sao10K/L3.3-70B-Euryale-v2.3** | ModÃ¨le communautaire avancÃ© | Applications spÃ©cialisÃ©es customisÃ©es |
| ğŸŒ  **Sao10K/L3.1-70B-Euryale-v2.2** | Variante communautaire | Applications spÃ©cifiques personnalisÃ©es |
| ğŸ’« **Sao10K/L3-70B-Euryale-v2.1** | Version prÃ©cÃ©dente Euryale | Applications nÃ©cessitant stabilitÃ© prouvÃ©e |
| ğŸŸ¡ **Qwen/Qwen2.5-7B-Instruct** | Qwen 2.5 compact | Applications lÃ©gÃ¨res multilingues |
| ğŸŒ  **NovaSky-AI/Sky-T1-32B-Preview** | ModÃ¨le communautaire expÃ©rimental | Applications innovantes |
| ğŸŒŒ **NousResearch/Hermes-3-Llama-3.1-405B** | MÃ©ga-modÃ¨le communautaire | Applications nÃ©cessitant qualitÃ© maximale |
| ğŸ”® **Gryphe/MythoMax-L2-13b** | ModÃ¨le crÃ©atif spÃ©cialisÃ© | Applications crÃ©atives, narration, fiction |
| ğŸ§™ **microsoft/WizardLM-2-8x22B** | ModÃ¨le Microsoft optimisÃ© | Applications professionnelles |
| ğŸ§© **deepseek-ai/DeepSeek-R1-Distill-Qwen-32B** | ModÃ¨le hybride distillÃ© | Applications Ã©quilibrÃ©es performance/taille |
| ğŸ”· **mistralai/Mistral-Small-24B-Instruct-2501** | Mistral intermÃ©diaire (24B) | Applications professionnelles Ã©quilibrÃ©es |
| ğŸ”± **deepseek-ai/DeepSeek-V3** | DerniÃ¨re gÃ©nÃ©ration DeepSeek | Applications premium nÃ©cessitant derniÃ¨res avancÃ©es |
| ğŸ§¬ **deepseek-ai/DeepSeek-R1-Distill-Llama-70B** | Grand modÃ¨le hybride | Applications premium avec spÃ©cificitÃ©s DeepSeek |
| ğŸŒŠ **deepseek-ai/DeepSeek-R1** | ModÃ¨le standard DeepSeek R1 | Applications professionnelles gÃ©nÃ©ralistes |
| âš¡ **deepseek-ai/DeepSeek-R1-Turbo** | Version optimisÃ©e pour vitesse | Applications nÃ©cessitant faible latence |
| ğŸ‘ï¸ **meta-llama/Llama-3.2-11B-Vision-Instruct** | Llama 3.2 multimodal (11B) | Applications traitant images et texte |
| ğŸ–¼ï¸ **meta-llama/Llama-3.2-90B-Vision-Instruct** | Grand Llama 3.2 multimodal | Applications multimodales premium |
| ğŸ’ **Qwen/Qwen2.5-72B-Instruct** | Grand modÃ¨le Qwen | Applications premium multilingues |
| âš¡ **nvidia/Llama-3.1-Nemotron-70B-Instruct** | Llama optimisÃ© par NVIDIA | Applications haute performance |
| ğŸ’» **Qwen/Qwen2.5-Coder-32B-Instruct** | SpÃ©cialisÃ© pour le code | Applications de dÃ©veloppement, gÃ©nÃ©ration de code |
| ğŸš€ **meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo** | Llama 3.1 optimisÃ© vitesse | Applications premium nÃ©cessitant faible latence |
| ğŸ”† **meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo** | Llama 3.1 compact rapide | Applications lÃ©gÃ¨res optimisÃ©es vitesse |
| ğŸ† **meta-llama/Meta-Llama-3.1-405B-Instruct** | MÃ©ga modÃ¨le Llama (405B) | Applications nÃ©cessitant qualitÃ© maximale |
| ğŸ”¹ **meta-llama/Meta-Llama-3.1-8B-Instruct** | Llama 3.1 compact | Applications lÃ©gÃ¨res Ã©quilibrÃ©es |
| ğŸ”¶ **meta-llama/Meta-Llama-3.1-70B-Instruct** | Llama 3.1 standard (70B) | Applications premium standard |
| ğŸ§  **microsoft/phi-4** | ModÃ¨le compact Microsoft | Applications efficientes, bon ratio performance/taille |
| ğŸ¦™ **meta-llama/Llama-3.3-70B-Instruct** | DerniÃ¨re gÃ©nÃ©ration Llama | Applications nÃ©cessitant derniÃ¨res innovations |
| âš¡ **meta-llama/Llama-3.3-70B-Instruct-Turbo** | Llama 3.3 optimisÃ© vitesse | Applications premium nÃ©cessitant faible latence |

---

## ğŸ› ï¸ Installation facile en 3 Ã©tapes

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/simonpierreboucher0/LLM-API.git
cd LLM-API
```

### 2ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configurer vos clÃ©s API
CrÃ©ez un fichier `.env` avec vos clÃ©s:
```ini
OPENAI_API_KEY=sk-xxxx
ANTHROPIC_API_KEY=sk-ant-xxxx
MISTRAL_API_KEY=xxxx
COHERE_API_KEY=xxxx
DEEPSEEK_API_KEY=xxxx
XAI_API_KEY=xxxx
GOOGLE_API_KEY=xxxx
QWEN_API_KEY=xxxx
AI21_API_KEY=xxxx
PERPLEXITY_API_KEY=pplx-xxxx
DEEPINFRA_API_KEY=xxxx
```

> ğŸ’¡ **Astuce**: Vous n'avez besoin de fournir que les clÃ©s pour les fournisseurs que vous allez utiliser!

---

## ğŸš€ Guide d'utilisation rapide

### â–¶ï¸ DÃ©marrer le serveur
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### ğŸ“‹ Obtenir la liste des modÃ¨les
```bash
curl -X GET http://localhost:8000/models
```

### ğŸ’¬ Chat simple avec GPT-4o
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Explique-moi les LLMs simplement."}],
    "temperature": 0.7
  }'
```

### ğŸŒŠ Chat en streaming avec Claude
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "anthropic",
    "model": "claude-3-opus-20240229",
    "messages": [{"role": "user", "content": "Ã‰cris un poÃ¨me sur l'IA."}],
    "stream": true,
    "system_message": "Tu es un poÃ¨te talentueux."
  }'
```

### ğŸ§  Chat avec raisonnement avancÃ© (o1)
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "o1",
    "messages": [{"role": "user", "content": "RÃ©sous ce problÃ¨me mathÃ©matique: Si 3x + 7 = 22, que vaut x?"}],
    "reasoning_effort": "high"
  }'
```

---

## ğŸ“Š Structure complÃ¨te des requÃªtes

```json
{
  "provider": "mistral",             // ğŸŒ Fournisseur LLM (obligatoire)
  "model": "mistral-large-latest",   // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "messages": [                      // ğŸ’¬ Historique de conversation (obligatoire)
    {"role": "user", "content": "Bonjour, comment Ã§a va?"}
  ],
  "temperature": 0.7,                // ğŸŒ¡ï¸ ContrÃ´le crÃ©ativitÃ© (0.0-1.0)
  "stream": false,                   // ğŸŒŠ Mode streaming (true/false)
  "max_tokens": 1024,                // ğŸ“ Longueur max de rÃ©ponse
  "system_message": "Tu es un assistant franÃ§ais.", // ğŸ§© Message systÃ¨me
  "stop": ["\n\n", "Fin:"],          // ğŸ›‘ SÃ©quences d'arrÃªt
  "top_p": 1.0,                      // ğŸ“ˆ Sampling de top-p
  "frequency_penalty": 0.0,          // ğŸ”„ PÃ©nalitÃ© rÃ©pÃ©titions
  "presence_penalty": 0.0,           // ğŸ“ PÃ©nalitÃ© prÃ©sence
  "response_format": {"type": "text"}, // ğŸ“„ Format de rÃ©ponse
  "reasoning_effort": null           // ğŸ§  Pour modÃ¨les OpenAI spÃ©ciaux
}
```

---

## ğŸ’¡ Cas d'utilisation avancÃ©s

### ğŸ”„ Failover automatique entre modÃ¨les
Si un modÃ¨le est indisponible, basculez facilement vers un autre:

```python
providers_priority = ["openai", "anthropic", "mistral"]
models_priority = {
    "openai": "gpt-4o",
    "anthropic": "claude-3-opus-20240229",
    "mistral": "mistral-large-latest"
}

for provider in providers_priority:
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            json={
                "provider": provider,
                "model": models_priority[provider],
                "messages": [{"role": "user", "content": "Explique l'IA quantique"}]
            }
        )
        if response.status_code == 200:
            print(f"SuccÃ¨s avec {provider}/{models_priority[provider]}")
            print(response.json())
            break
    except Exception as e:
        print(f"Ã‰chec avec {provider}: {str(e)}")
```

### ğŸ” A/B Testing entre diffÃ©rents modÃ¨les
Comparez facilement les performances de diffÃ©rents modÃ¨les:

```python
test_prompt = "Explique la thÃ©orie de la relativitÃ© simplement."
models_to_test = [
    {"provider": "openai", "model": "gpt-4o"},
    {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022"},
    {"provider": "mistral", "model": "mistral-large-latest"},
    {"provider": "google", "model": "gemini-1.5-pro"}
]

results = {}
for model_info in models_to_test:
    provider = model_info["provider"]
    model = model_info["model"]
    
    start_time = time.time()
    response = requests.post(
        "http://localhost:8000/v1/chat/completions",
        json={
            "provider": provider,
            "model": model,
            "messages": [{"role": "user", "content": test_prompt}]
        }
    )
    end_time = time.time()
    
    if response.status_code == 200:
        data = response.json()
        content = data["choices"][0]["message"]["content"]
        latency = end_time - start_time
        tokens = data.get("usage", {}).get("completion_tokens", 0)
        
        results[f"{provider}/{model}"] = {
            "latency": latency,
            "tokens": tokens,
            "content": content[:100] + "..." # Afficher seulement le dÃ©but
        }

# Afficher les rÃ©sultats
for model_name, result in results.items():
    print(f"ğŸ“Š {model_name}:")
    print(f"â±ï¸ Latence: {result['latency']:.2f}s")
    print(f"ğŸ”¢ Tokens: {result['tokens']}")
    print(f"ğŸ“ DÃ©but: {result['content']}")
    print("---")
```

---

## ğŸ§ª Tests complets

Notre script de test permet de vÃ©rifier tous les modÃ¨les:

```bash
python test_llm_gateway.py
```

Le script exÃ©cute:
- âœ… Tests non-streaming pour chaque fournisseur
- ğŸŒŠ Tests de streaming pour les modÃ¨les compatibles
- ğŸ“Š Validation des formats de rÃ©ponse
- ğŸ“ GÃ©nÃ©ration d'un rapport de test dÃ©taillÃ©

```
=== RÃ‰SUMÃ‰ DES TESTS NON-STREAMING ===
openai/gpt-4o: âœ… SuccÃ¨s
anthropic/claude-3-5-sonnet-20241022: âœ… SuccÃ¨s
mistral/mistral-large-latest: âœ… SuccÃ¨s
cohere/c4ai-aya-expanse-32b: âœ… SuccÃ¨s
...

=== RÃ‰SUMÃ‰ DES TESTS STREAMING ===
openai/gpt-4o: âœ… SuccÃ¨s
anthropic/claude-3-5-sonnet-20241022: âœ… SuccÃ¨s
mistral/mistral-large-latest: âœ… SuccÃ¨s
...
```

---

## ğŸ”’ SÃ©curitÃ© et bonnes pratiques

- ğŸ”‘ **Gestion des clÃ©s**: StockÃ©es localement dans `.env`, jamais exposÃ©es
- ğŸ›¡ï¸ **CORS**: Configuration sÃ©curisÃ©e pour les requÃªtes cross-origin
- ğŸ“ **Logging**: DÃ©tails utiles sans informations sensibles
- ğŸ”„ **Rate Limiting**: Respect des limites de taux des fournisseurs
- ğŸ§¹ **Nettoyage**: Aucune donnÃ©e stockÃ©e sur le serveur

---

## ğŸ“š Guide des fonctionnalitÃ©s avancÃ©es

### ğŸ§© Messages multimodaux
Certains modÃ¨les supportent l'envoi d'images:

```json
{
  "provider": "openai",
  "model": "gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Que vois-tu sur cette image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://example.com/image.jpg"
          }
        }
      ]
    }
  ]
}
```

### ğŸš€ ParamÃ¨tres spÃ©cifiques aux modÃ¨les

#### ğŸ§  Effort de raisonnement pour OpenAI o1/o3
```json
{
  "provider": "openai",
  "model": "o1",
  "messages": [{"role": "user", "content": "RÃ©sous ce problÃ¨me complexe..."}],
  "reasoning_effort": "high"  // Options: "auto", "low", "medium", "high"
}
```

#### ğŸ”„ Format de rÃ©ponse JSON
```json
{
  "provider": "openai",
  "model": "gpt-4o",
  "messages": [{"role": "user", "content": "Liste 3 fruits avec leurs caractÃ©ristiques"}],
  "response_format": {"type": "json_object"}
}
```

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues! Voici comment participer:

1. ğŸ´ **Fork** le dÃ©pÃ´t
2. ğŸ”„ CrÃ©ez une **branche** (`git checkout -b feature/ma-fonctionnalite`)
3. âœï¸ Faites vos **modifications**
4. ğŸ“¦ **Commit** vos changements (`git commit -m 'Ajout de ma fonctionnalitÃ©'`)
5. ğŸ“¤ **Push** vers la branche (`git push origin feature/ma-fonctionnalite`)
6. ğŸ” Ouvrez une **Pull Request**

### ğŸ’¼ IdÃ©es de contributions

- ğŸ§ª Ajout de tests supplÃ©mentaires
- ğŸ“ AmÃ©lioration de la documentation
- âœ¨ Support de nouveaux fournisseurs
- ğŸš€ Optimisations de performance
- ğŸŒ Internationalisation

---

## ğŸ“œ Licence

Ce projet est sous licence [MIT](LICENSE) - voir le fichier LICENSE pour plus de dÃ©tails.

---

## â“ FAQ

### ğŸ”„ Comment mettre Ã  jour les modÃ¨les disponibles?
Modifiez le dictionnaire `MODELS` dans le code pour ajouter ou mettre Ã  jour les modÃ¨les.

### ğŸ”‘ Comment gÃ©rer plusieurs clÃ©s API pour le mÃªme fournisseur?
Vous pouvez Ã©tendre la classe `APIKeys` pour implÃ©menter une rotation des clÃ©s.

### ğŸŒ Puis-je exÃ©cuter l'API derriÃ¨re un proxy?
Oui, utilisez les variables d'environnement `HTTP_PROXY` et `HTTPS_PROXY`.

### ğŸ“± Est-ce compatible avec les applications mobiles?
Oui, l'API peut Ãªtre utilisÃ©e par n'importe quel client HTTP.

### ğŸ’¸ Comment optimiser les coÃ»ts?
Utilisez les modÃ¨les plus petits pour les tÃ¢ches simples et rÃ©servez les grands modÃ¨les pour les tÃ¢ches complexes.

---

## ğŸ‘¨â€ğŸ’» Auteurs

- ğŸš€ [Simon-Pierre Boucher](https://github.com/simonpierreboucher0) - CrÃ©ateur principal

---

<p align="center">
â­ N'oubliez pas de mettre une Ã©toile si ce projet vous a Ã©tÃ© utile! â­
</p>

<p align="center">
ğŸ”— <a href="https://github.com/simonpierreboucher0/LLM-API">GitHub</a> | 
ğŸ› <a href="https://github.com/simonpierreboucher0/LLM-API/issues">Signaler un problÃ¨me</a> | 
ğŸ’¡ <a href="https://github.com/simonpierreboucher0/LLM-API/discussions">Discussions</a>
</p>
