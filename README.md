# LLM API Gateway

![LLM API Gateway Banner](https://github.com/simonpierreboucher0/LLM-API/raw/main/assets/banner.png)

## üìå Vue d'ensemble

LLM API Gateway est une API unifi√©e qui vous permet d'acc√©der √† plus de 100 mod√®les de langage (LLMs) √† travers 11 fournisseurs diff√©rents via une interface standardis√©e. Plus besoin de g√©rer s√©par√©ment les multiples formats d'API et les sp√©cificit√©s de chaque fournisseur.

[![GitHub stars](https://img.shields.io/github/stars/simonpierreboucher0/LLM-API?style=social)](https://github.com/simonpierreboucher0/LLM-API/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.105.0+-green.svg)](https://fastapi.tiangolo.com/)

## üî• Caract√©ristiques principales

- **Interface unifi√©e**: Utilisez la m√™me syntaxe pour tous les mod√®les
- **Normalisation automatique**: Les requ√™tes et r√©ponses sont automatiquement adapt√©es √† chaque fournisseur
- **Compatibilit√© OpenAI**: Format de r√©ponse standardis√© compatible avec l'√©cosyst√®me OpenAI
- **Support du streaming**: R√©ponses en temps r√©el pour tous les mod√®les qui le supportent
- **Gestion des sp√©cificit√©s**: Adaptation automatique aux particularit√©s de chaque mod√®le
- **Syst√®me de logging**: Suivi d√©taill√© des requ√™tes et des erreurs pour faciliter le d√©bogage

## üåê Fournisseurs et mod√®les pris en charge

| Fournisseur | Mod√®les cl√©s | Description | Particularit√©s |
|-------------|--------------|-------------|----------------|
| **OpenAI** | GPT-4o, GPT-3.5-Turbo, o1, o3-mini | Leader du march√© avec une large gamme de mod√®les | Mod√®les sp√©ciaux (o1, o3) avec param√®tres sp√©cifiques |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus, Haiku | Mod√®les optimis√©s pour la s√©curit√© et l'alignement | Format de message syst√®me distinct |
| **Mistral AI** | Mistral Large, Small, Saba, Codestral | Mod√®les open et closed source avec excellentes performances | Excellents mod√®les de code |
| **Cohere** | Command, Command-R, Aya | Sp√©cialis√©s dans la r√©cup√©ration d'informations | Format de contenu sp√©cifique |
| **Google** | Gemini 1.5 Pro, Flash | Mod√®les multimodaux avanc√©s | Utilise un format de message diff√©rent |
| **DeepSeek** | DeepSeek Chat, Reasoner | Mod√®les sp√©cialis√©s dans le raisonnement | Compatible format OpenAI |
| **xAI** | Grok-2 | Mod√®le conversationnel avanc√© | Similaire au format OpenAI |
| **Qwen** | Qwen2.5, Qwen1.5 | Mod√®les multilingues d'Alibaba | Bonne performance en mandarin |
| **AI21** | Jamba-1.5 | Mod√®les sp√©cialis√©s en g√©n√©ration structur√©e | Supportent des contraintes avanc√©es |
| **Perplexity** | Sonar, Reasoning Pro | Mod√®les optimis√©s pour la recherche | Excellents pour la synth√®se d'informations |
| **DeepInfra** | Mod√®les Mixtral, Llama, Mistral... | H√©bergement de nombreux mod√®les open-source | Grande vari√©t√© de mod√®les disponibles |

## üõ†Ô∏è Installation

### Pr√©requis

- Python 3.8 ou sup√©rieur
- pip (gestionnaire de paquets Python)
- Cl√©s API pour les fournisseurs que vous souhaitez utiliser

### √âtapes d'installation

1. **Cloner le d√©p√¥t**

```bash
git clone https://github.com/simonpierreboucher0/LLM-API.git
cd LLM-API
```

2. **Installer les d√©pendances**

```bash
pip install -r requirements.txt
```

3. **Configurer les cl√©s API**

Cr√©ez un fichier `.env` √† la racine du projet avec vos cl√©s API:

```ini
OPENAI_API_KEY=sk-xxxx
ANTHROPIC_API_KEY=sk-ant-xxxx
MISTRAL_API_KEY=xxxx
COHERE_API_KEY=xxxx
DEEPSEEK_API_KEY=xxxx
XAI_API_KEY=xxxx
GOOGLE_API_KEY=xxxx
QWEN_API_KEY=xxxx
AI21_API_KEY=xxxx
PERPLEXITY_API_KEY=pplx-xxxx
DEEPINFRA_API_KEY=xxxx
```

> **Note**: Vous n'avez besoin de fournir que les cl√©s API pour les fournisseurs que vous pr√©voyez d'utiliser.

## üöÄ Utilisation

### D√©marrer le serveur

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Une fois le serveur d√©marr√©, vous pouvez acc√©der √† la documentation interactive √† l'adresse:
```
http://localhost:8000/docs
```

### Exemples de requ√™tes

#### 1. Obtenir la liste des mod√®les disponibles

```bash
curl -X GET http://localhost:8000/models
```

#### 2. Chat standard avec GPT-4o

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Explique la th√©orie de la relativit√© simplement."}],
    "temperature": 0.7,
    "stream": false,
    "max_tokens": 500
  }'
```

#### 3. Chat en streaming avec Claude

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "anthropic",
    "model": "claude-3-opus-20240229",
    "messages": [{"role": "user", "content": "√âcris une histoire courte sur l'intelligence artificielle."}],
    "temperature": 0.9,
    "stream": true,
    "max_tokens": 800,
    "system_message": "Tu es un √©crivain cr√©atif sp√©cialis√© en science-fiction."
  }'
```

#### 4. Chat avec le mod√®le sp√©cial o1 d'OpenAI

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "o1",
    "messages": [{"role": "user", "content": "Explique le fonctionnement des transformers en d√©tail."}],
    "max_tokens": 2000,
    "reasoning_effort": "high"
  }'
```

#### 5. Chat avec contraintes de format

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "mistral",
    "model": "mistral-large-latest",
    "messages": [{"role": "user", "content": "G√©n√®re un rapport sur les tendances IA en 2024."}],
    "temperature": 0.5,
    "response_format": {"type": "text"},
    "stop": ["CONCLUSION:", "Fin du rapport"],
    "system_message": "Tu es un analyste expert en intelligence artificielle."
  }'
```

## üìã Structure compl√®te des requ√™tes

```json
{
  "provider": "openai",             // Le fournisseur LLM (obligatoire)
  "model": "gpt-4o",                // Le mod√®le sp√©cifique (obligatoire)
  "messages": [                     // L'historique de conversation (obligatoire)
    {"role": "user", "content": "Bonjour, comment √ßa va?"}
  ],
  "temperature": 0.7,               // Contr√¥le de la cr√©ativit√© (0.0-1.0)
  "stream": false,                  // Mode streaming (true/false)
  "max_tokens": 1024,               // Longueur maximale de la r√©ponse
  "system_message": "Tu es un assistant informatique utile et pr√©cis.",  // Message syst√®me
  "stop": ["\n\n", "Conclusion:"],  // S√©quences d'arr√™t
  "top_p": 1.0,                     // Sampling de top-p
  "frequency_penalty": 0.0,         // P√©nalit√© pour les r√©p√©titions fr√©quentes
  "presence_penalty": 0.0,          // P√©nalit√© pour les r√©p√©titions de sujets
  "response_format": {"type": "text"}, // Format de r√©ponse
  "reasoning_effort": null          // Uniquement pour les mod√®les OpenAI sp√©ciaux
}
```

## üí° Fonctionnalit√©s d√©taill√©es

### Formats de messages

L'API prend en charge plusieurs formats de contenu pour les messages:

1. **Format texte simple**:
```json
{"role": "user", "content": "Bonjour, comment √ßa va?"}
```

2. **Format avec objets de contenu** (pour les contenus multimodaux):
```json
{"role": "user", "content": [{"type": "text", "text": "Que repr√©sente cette image?"}, {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}]}
```

### Normalisation des messages

Le syst√®me convertit automatiquement les formats de messages en fonction des exigences de chaque fournisseur:

- Pour OpenAI standard: Format compatible OpenAI
- Pour les mod√®les sp√©ciaux d'OpenAI (o1, o3): Format avec r√¥le "developer" pour les messages syst√®me
- Pour Anthropic: Le message syst√®me est g√©r√© s√©par√©ment
- Pour Google: Conversion des r√¥les et structuration sp√©cifique des "parts"

### Gestion du streaming

Le streaming est g√©r√© diff√©remment selon les fournisseurs:

- Les mod√®les sp√©ciaux d'OpenAI (o1, o3) ne supportent pas le streaming
- Les r√©ponses de streaming sont normalis√©es au format OpenAI pour une compatibilit√© maximale
- Les formats SSE (Server-Sent Events) sp√©cifiques sont trait√©s automatiquement

### Param√®tres sp√©cifiques aux mod√®les

- `reasoning_effort`: Uniquement pour les mod√®les OpenAI sp√©ciaux (o1, o3), contr√¥le la profondeur du raisonnement
- Les mod√®les sp√©ciaux d'OpenAI n'acceptent que `temperature=1.0`
- Certains fournisseurs utilisent des param√®tres sp√©cifiques qui sont automatiquement convertis

## üîç Architecture technique

### Composants principaux

1. **Normalisation des requ√™tes**: Convertit les messages au format sp√©cifique de chaque fournisseur
2. **Transformation des requ√™tes**: Adapte les param√®tres et en-t√™tes pour chaque API
3. **Traitement du streaming**: G√®re les diff√©rents formats de streaming et normalise les r√©ponses
4. **Normalisation des r√©ponses**: Convertit toutes les r√©ponses en un format unifi√© compatible OpenAI

### Flux de traitement

1. La requ√™te est re√ßue par l'API Gateway
2. Le `provider` et le `model` sont valid√©s
3. Les messages sont normalis√©s selon le format requis par le fournisseur
4. La requ√™te est transform√©e avec les en-t√™tes et param√®tres appropri√©s
5. La requ√™te est envoy√©e √† l'API du fournisseur
6. La r√©ponse (streaming ou non) est normalis√©e
7. La r√©ponse normalis√©e est renvoy√©e au client

### Gestion des erreurs

- Les erreurs sp√©cifiques aux fournisseurs sont captur√©es et normalis√©es
- Les exceptions sont propag√©es avec des messages d'erreur clairs
- Le syst√®me de logging enregistre les d√©tails pour faciliter le d√©bogage

## üß™ Tests

Un script de test est fourni pour v√©rifier la compatibilit√© avec tous les mod√®les:

```bash
python test_llm_gateway.py
```

Ce script va:
- Tester la connectivit√© avec chaque fournisseur
- V√©rifier le bon fonctionnement du mode non-streaming
- V√©rifier le bon fonctionnement du mode streaming (pour les mod√®les qui le supportent)
- G√©n√©rer un rapport des r√©sultats

## üöß Limitations connues

- **Mod√®les sp√©ciaux d'OpenAI**: Les mod√®les o1 et o3-mini ne supportent pas le streaming et ont des restrictions de temp√©rature
- **Formats multimodaux**: La prise en charge des images varie selon les fournisseurs
- **Fonction calling**: Actuellement non support√©e de mani√®re unifi√©e
- **Outils et plugins**: Non support√©s actuellement
- **Quotas et limites**: Les limites de rate sont g√©r√©es par les fournisseurs individuels

## üîí S√©curit√©

- Les cl√©s API sont stock√©es localement dans le fichier `.env`
- Les cl√©s ne sont jamais expos√©es dans les r√©ponses
- CORS est configur√© pour permettre les requ√™tes cross-origin
- Aucune donn√©e n'est stock√©e sur le serveur

## üìö En savoir plus

### Diff√©rences entre les fournisseurs

1. **OpenAI**:
   - Large gamme de mod√®les pour diff√©rents cas d'utilisation
   - Mod√®les sp√©ciaux (o1, o3) avec capacit√©s de raisonnement avanc√©es
   - Bonne gestion du multimodal

2. **Anthropic**:
   - Mod√®les con√ßus pour la s√©curit√© et l'alignement
   - Bonnes capacit√©s de raisonnement et de r√©sum√©
   - API l√©g√®rement diff√©rente pour les messages syst√®me

3. **Mistral AI**:
   - Excellentes performances pour la taille des mod√®les
   - Mod√®les open-source et propri√©taires
   - Bons mod√®les pour le code et le raisonnement

4. **Google**:
   - Mod√®les Gemini avec excellentes capacit√©s multimodales
   - Structure d'API tr√®s diff√©rente des autres
   - Bonnes performances sur les t√¢ches complexes

### Cas d'utilisation recommand√©s

- **D√©veloppement et prototypage**: Testez facilement plusieurs mod√®les sans changer votre code
- **Failover automatique**: Passez d'un fournisseur √† un autre en cas d'indisponibilit√©
- **Optimisation des co√ªts**: Utilisez le mod√®le le plus √©conomique adapt√© √† chaque t√¢che
- **Applications multi-mod√®les**: Combinez diff√©rents mod√®les selon leurs forces

## ü§ù Contribution

Les contributions sont les bienvenues! Voici comment vous pouvez aider:

1. Fork le d√©p√¥t
2. Cr√©ez une branche pour votre fonctionnalit√© (`git checkout -b feature/ma-fonctionnalite`)
3. Committez vos changements (`git commit -m 'Ajout de ma fonctionnalit√©'`)
4. Poussez vers la branche (`git push origin feature/ma-fonctionnalite`)
5. Ouvrez une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üë• Auteurs et remerciements

- [Simon-Pierre Boucher](https://github.com/simonpierreboucher0) - Cr√©ateur et mainteneur principal
- Merci √† tous les contributeurs et √† la communaut√© open-source

---

## FAQ

### Comment ajouter une nouvelle cl√© API?
Ajoutez simplement votre cl√© dans le fichier `.env` avec le format appropri√© pour le fournisseur.

### Puis-je utiliser cette API avec des applications frontend?
Oui, l'API est configur√©e avec CORS pour permettre les requ√™tes depuis n'importe quelle origine.

### Comment g√©rer les limites de taux (rate limits)?
L'API transmet les limites de taux des fournisseurs sous-jacents. Si vous atteignez une limite, l'erreur correspondante sera renvoy√©e.

### Est-ce que tous les mod√®les supportent les m√™mes fonctionnalit√©s?
Non, chaque mod√®le a ses propres capacit√©s. Par exemple, certains mod√®les ne supportent pas le streaming ou ont des limitations de param√®tres sp√©cifiques.

### Comment puis-je ajouter un nouveau fournisseur?
Pour ajouter un nouveau fournisseur, vous devez:
1. Ajouter le fournisseur √† l'√©num√©ration `Provider`
2. D√©finir les mod√®les disponibles dans `MODELS`
3. Cr√©er une fonction de transformation pour ce fournisseur
4. Ajouter la gestion sp√©cifique dans les fonctions de normalisation

---

Cr√©√© par [Simon-Pierre Boucher](https://github.com/simonpierreboucher0) | [Signaler un probl√®me](https://github.com/simonpierreboucher0/LLM-API/issues)
